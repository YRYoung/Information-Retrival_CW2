{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b598f34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from utils import queries_embeddings, load_passages_tensors, train_raw_df\n",
    "from NN.CustomDataset import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3644ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:37<00:00,  3.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "p_tensors_all = load_passages_tensors()\n",
    "'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33607fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def BCEloss(y, h):\n",
    "    return y * np.log(h) + (1 - y) * np.log(1 - h)\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "    def __init__(self, learning_rate, n_iterations):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.accuracies = []\n",
    "        self.continue_training = False\n",
    "        self.losses = np.array([])\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if self.w is None and self.b is None:\n",
    "            self.w = np.zeros(600)\n",
    "            self.b = 0\n",
    "\n",
    "        self.losses = np.concatenate([self.losses, np.zeros(self.n_iterations)])\n",
    "\n",
    "    def fit(self, dataloader, evaluator=None):\n",
    "        start_epoch = len(self.losses)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        # gradient descent\n",
    "        for epoch in range(start_epoch, start_epoch + self.n_iterations):\n",
    "            ic(epoch)\n",
    "\n",
    "            pbar = tqdm(enumerate(dataloader), unit='batch',\n",
    "                        total=len(dataloader)) if verbose else enumerate(dataloader)\n",
    "            for i_batch, (x_batch, y_batch) in pbar:\n",
    "                loss = self._fit_batch(x_batch, y_batch)\n",
    "\n",
    "                if verbose:\n",
    "                    pbar.set_postfix({'loss': loss})\n",
    "            self.losses[epoch] = loss\n",
    "\n",
    "            if evaluator is not None:\n",
    "                self.accuracies.append(evaluator(self.forward))\n",
    "\n",
    "        self.get_history()\n",
    "        print(\"done\")\n",
    "\n",
    "    def _fit_batch(self, x, y):\n",
    "        #         weights=np.int(y==1)\n",
    "        n = x.shape[0]\n",
    "        h = self.forward(x)\n",
    "        tmp = h - y\n",
    "\n",
    "        dw = x.T.dot(tmp)\n",
    "        db = np.einsum('i->', tmp)\n",
    "\n",
    "        scaler = self.learning_rate / n\n",
    "        self.w -= dw * scaler\n",
    "        self.b -= db * scaler\n",
    "\n",
    "        return - np.einsum('i->', BCEloss(y, h)) / n\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = sigmoid(x.dot(self.w) + self.b)\n",
    "        return res\n",
    "\n",
    "    def save(self, path):\n",
    "        return torch.save({'w': self.w, \"b\": self.b, 'history': self.history,\n",
    "                           'losses': self.losses, 'accuracies': self.accuracies}, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        value = torch.load(path)\n",
    "        self.w = value['w']\n",
    "        self.b = value['b']\n",
    "        self.losses = value['losses']\n",
    "        self.losses = self.losses[self.losses != 0]\n",
    "        self.accuracies = value['accuracies']\n",
    "        self.history = value['history']\n",
    "        self.continue_training = True\n",
    "\n",
    "    def get_history(self):\n",
    "        result_df = pd.DataFrame(self.losses, columns=['Loss'])\n",
    "        result_df.loc[:, ['mAP@3', 'mAP@10', 'mAP@100']] = [a[0] for a in self.accuracies]\n",
    "        result_df.loc[:, ['NDCG@3', 'NDCG@10', 'NDCG@100']] = [a[1] for a in self.accuracies]\n",
    "        self.history = result_df.iloc[:, [1, 2, 3, 4, 5, 6, 0]]\n",
    "        return self.history\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, batch_size: int, passages_per_query: int, p_tensors=None, dataframe=None, q_tensors=None, ):\n",
    "\n",
    "        if p_tensors is None:\n",
    "            p_tensors = load_passages_tensors()\n",
    "\n",
    "        if q_tensors is None:\n",
    "            q_tensors = torch.load(queries_embeddings, map_location=torch.device('cpu'))\n",
    "\n",
    "        if dataframe is None:\n",
    "            dataframe = pd.read_parquet(train_raw_df)\n",
    "\n",
    "        self.dataset = CustomDataset(all_dataframe=dataframe,\n",
    "                                     passages_tensors=p_tensors,\n",
    "                                     queries_tensors=q_tensors,\n",
    "                                     passages_per_query=passages_per_query,\n",
    "                                     return_tensors='cat',\n",
    "                                     shuffle_passages=False)\n",
    "\n",
    "        self.passages_per_query = passages_per_query\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = len(self.dataset) // self.batch_size + 1\n",
    "\n",
    "        ic('DataLoader', len(self.dataset), self.num_batches, self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        for start in range(0, len(self.dataset), self.batch_size):\n",
    "            end = min(start + self.batch_size, len(self.dataset))\n",
    "            this_batch_size = end - start\n",
    "\n",
    "            x = np.zeros((this_batch_size * self.passages_per_query, 2, 300))\n",
    "            y = np.zeros(this_batch_size * self.passages_per_query)\n",
    "            for indice, q_idx in enumerate(range(start, end)):\n",
    "                xx, yy = self.dataset[q_idx]\n",
    "                idx_start = indice * self.passages_per_query\n",
    "                idx_end = idx_start + self.passages_per_query\n",
    "\n",
    "                x[idx_start:idx_end, ...] = xx\n",
    "                y[idx_start:idx_end] = yy\n",
    "\n",
    "            yield x.reshape(-1, 600), y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c750934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import eval\n",
    "\n",
    "evaluator=eval.init_evaluator(\n",
    "    x_val_handler=lambda x: x.numpy().reshape(-1, 600))\n",
    "\n",
    "\n",
    "q_tensors = torch.load(queries_embeddings, map_location=torch.device('cpu'))\n",
    "dataframe = pd.read_parquet(train_raw_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae6eeb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| 'DataLoader': 'DataLoader'\n",
      "    len(self.dataset): 4521\n",
      "    self.num_batches: 18\n",
      "    self.batch_size: 256\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(batch_size=256, passages_per_query=20,\n",
    "                        p_tensors=p_tensors_all, dataframe=dataframe,\n",
    "                        q_tensors=q_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3d202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# name = './nsample15_100_5'\n",
    "model = LogisticRegression(learning_rate=10, n_iterations=200)\n",
    "# model.load(f'{name}.pth')\n",
    "model.fit(dataloader,evaluator)\n",
    "\n",
    "name = './nsample20_200_10'\n",
    "model.save(f'{name}.pth')\n",
    "\n",
    "dff = model.get_history()\n",
    "dff.to_parquet(f'{name}.dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name='./nsample15_100_10'\n",
    "\n",
    "model = LogisticRegression(learning_rate=30, n_iterations=200)\n",
    "\n",
    "model.fit(dataloader,evaluator)\n",
    "\n",
    "name='./nsample20_200_30'\n",
    "model.save(f'{name}.pth')\n",
    "\n",
    "dff = model.get_history()\n",
    "dff.to_parquet(f'{name}.dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae6847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name='./nsample15_100_10'\n",
    "\n",
    "model = LogisticRegression(learning_rate=.5, n_iterations=300)\n",
    "\n",
    "model.fit(dataloader,evaluator)\n",
    "\n",
    "name='./nsample50_300_.5'\n",
    "model.save(f'{name}.pth')\n",
    "\n",
    "dff = model.get_history()\n",
    "dff.to_parquet(f'{name}.dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad778541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=40, n_iterations=200)\n",
    "\n",
    "model.fit(dataloader,evaluator)\n",
    "\n",
    "name='./debug_200_40'\n",
    "model.save(f'{name}.pth')\n",
    "\n",
    "dff = model.get_history()\n",
    "dff.to_parquet(f'{name}.dataframe')\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "model2 = LogisticRegression(learning_rate=0.005, n_iterations=200)\n",
    "# model2.load('./debug_200_0.005.pth')\n",
    "model2.fit(dataloader,evaluator)\n",
    "\n",
    "name='./debug_200_0.005'\n",
    "model2.save(f'{name}.pth')\n",
    "\n",
    "dff2 = model2.get_history()\n",
    "dff2.to_parquet(f'{name}.dataframe')\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "model2 = LogisticRegression(learning_rate=0.02, n_iterations=200)\n",
    "# model2.load('./debug_400_0.02.pth')\n",
    "model2.fit(dataloader,evaluator)\n",
    "\n",
    "name='./debug_200_0.02'\n",
    "model2.save(f'{name}.pth')\n",
    "\n",
    "dff2 = model2.get_history()\n",
    "dff2.to_parquet(f'{name}.dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0fd048",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(learning_rate=15, n_iterations=400)\n",
    "model2.load('./debug_200_15.pth')\n",
    "model2.fit(dataloader,evaluator)\n",
    "\n",
    "name='./debug_400_15'\n",
    "model2.save(f'{name}.pth')\n",
    "\n",
    "dff2 = model2.get_history()\n",
    "dff2.to_parquet(f'{name}.dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7efe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def send(message):\n",
    "    bot_token = '6028862035:AAF_oOGGHuJL0CSPBCXzNkYlyzrV2uWqh9Y'\n",
    "    bot_chatId = '5287678337'\n",
    "\n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={bot_chatId}&text={message}\"\n",
    "    a = requests.get(url).json()\n",
    "\n",
    "\n",
    "send('LR finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcf1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
