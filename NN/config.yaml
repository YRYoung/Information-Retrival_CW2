-
  CNN:
    activation: leaky_relu
    batchNorm:
      - 1
      - 1
      - 1
    denseUnit:
      - 512
      - 128
    kernels:
      - 3
      - 3
      - 3
    layers:
      - 16
      - 32
      - 32
    maxPool:
      - 1
      - 1
      - 0
    stride:
      - 1
      - 1
      - 1
  general:
    displayNet: false
    seed: 0
  training:
    attention:
      - true
      - false
    2CNN: false
    bce : 0.1
    cv: 14
    cat_qp: diff
    decay: -7
    dropRate: 0
    learning_rate: 5.e-3
    init_epoch: -1
    train: true
    batch_size: 256
    passages_per_query: 200
    list_mle: 0.5
  note: 'add attention at the top only one cnn'
